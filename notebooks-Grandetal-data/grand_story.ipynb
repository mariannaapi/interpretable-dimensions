{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6602ae-d174-4817-bc78-b51b819d19e9",
   "metadata": {},
   "source": [
    "# Experiments on the data of Grand et al"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031bdfad-e63a-4f3c-95c2-6f0f024e2818",
   "metadata": {},
   "source": [
    "# Global data, set to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac626d07-7e3f-429d-9b00-a1e58cae4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"../glove/glove.42B.300d.zip\"\n",
    "glove_file = \"glove.42B.300d.txt\"\n",
    "\n",
    "grandratings_dir = \"../data/Grandetal-data/\"\n",
    "grandfeatures_path = \"../data/Grandetal-data/features.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26d07d-a1a2-40a6-8c30-d328c8900b24",
   "metadata": {},
   "source": [
    "## Loading stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1dfe0-29ac-4db1-ac31-0eb90506ea8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import stats\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import math\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "feature_dim = 300\n",
    "\n",
    "word_vectors = { }\n",
    "\n",
    "with zipfile.ZipFile(glove_path) as azip:\n",
    "    with azip.open(glove_file) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0].decode()\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            word_vectors[word] = vector\n",
    "\n",
    "grandfeatures_df = pd.read_excel(grandfeatures_path)\n",
    "\n",
    "# reading in Grand data\n",
    "def read_grand_data(filename, grandratings_dir, grandfeatures_df):\n",
    "    # extract category and feature\n",
    "    grandcategory, grandfeature = filename[:-4].split(\"_\")\n",
    "        \n",
    "    # read human ratings, make gold column\n",
    "    df = pd.read_csv(grandratings_dir + filename)\n",
    "    nspeakers = len(df.columns) -1\n",
    "    df[\"Average\"] = [row.iloc[1:26].sum() / nspeakers for _, row in df.iterrows()]\n",
    "    # z-scores of average ratings\n",
    "    df[\"Gold\"] = (df[\"Average\"] - df[\"Average\"].mean()) / df[\"Average\"].std()\n",
    "        \n",
    "    # obtain seed words from excel file\n",
    "    relevant_row = grandfeatures_df[grandfeatures_df.Dimension == grandfeature]\n",
    "    seedwords = relevant_row.iloc[:, 1:].values.flatten().tolist()\n",
    "    pos_seedwords = seedwords[:3]\n",
    "    neg_seedwords = seedwords[3:]\n",
    "    \n",
    "    return (grandcategory, grandfeature, pos_seedwords, neg_seedwords, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090dda3f-040d-4141-9f95-dd983be77e1e",
   "metadata": {},
   "source": [
    "## Reproducing their results\n",
    "\n",
    "We reproduce their results from the Nature paper almost perfectly, on both Pearson's r correlation and pairwise order evaluation OC_P. X percent of category/feature pairs show a significant correlation, and average OC_P is Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2945e83-e61a-483f-b5f1-94d34f867a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script for running on data\n",
    "def each_grandcondition(grandratings_dir, grandfeatures_df):\n",
    "    for filename in os.listdir(grandratings_dir): \n",
    "        if not filename.endswith(\"csv\"):\n",
    "            continue\n",
    "\n",
    "        grandcategory, grandfeature, pos_seedwords, neg_seedwords, df = read_grand_data(filename, \n",
    "                                                                    grandratings_dir, \n",
    "                                                                    grandfeatures_df)\n",
    "\n",
    "        # storage for word vectors and gold values for this dataset\n",
    "        data_vectors = []\n",
    "\n",
    "        # collect word vectors and gold ratings\n",
    "        for row in df.itertuples():\n",
    "            # row.Row is the word. look it up in word_vectors\n",
    "            data_vectors.append( word_vectors[ row.Row ])\n",
    "            \n",
    "        yield (grandcategory, grandfeature, pos_seedwords, neg_seedwords, df, data_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8637842-e4aa-42f3-bf45-db25445ff1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import compute_dim\n",
    "import eval_dim\n",
    "from scipy import stats\n",
    "\n",
    "results = [ ]\n",
    "\n",
    "for grandcategory, grandfeature, pos_seedwords, neg_seedwords, df, data_vectors in each_grandcondition(grandratings_dir, grandfeatures_df):\n",
    "    \n",
    "    dimension = compute_dim.dimension_seedbased(pos_seedwords, neg_seedwords, word_vectors)\n",
    "    \n",
    "    df[\"Pred\"]  = compute_dim.predict_scalarproj(data_vectors, dimension)\n",
    "    \n",
    "    ocp = eval_dim.pairwise_order_consistency(df[\"Gold\"], df[\"Pred\"])\n",
    "    result_obj = stats.pearsonr(df[\"Gold\"], df[\"Pred\"])\n",
    "    \n",
    "    results.append({\"category\": grandcategory,\n",
    "                    \"feature\" : grandfeature,\n",
    "                    \"ocp\" : ocp,\n",
    "                    \"pearsonr\" : result_obj.statistic,\n",
    "                    \"pvalue\" : result_obj.pvalue } )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efba4d-c057-4484-8b6f-b028ff4e31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arranging the data in the same order as in the Grand et al paper so we can compare numbers\n",
    "\n",
    "for r in sorted(results, key = lambda r:(r[\"feature\"], r[\"category\"])):\n",
    "    starred = \"*\" if r[\"pearsonr\"] > 0 and r[\"pvalue\"] < 0.05 else \"\"\n",
    "    print(r[\"category\"], r[\"feature\"], \n",
    "          \"r\", round(r[\"pearsonr\"], 3), \"p=\", round(r[\"pvalue\"],3), \n",
    "          \"ocp\", round(r[\"ocp\"], 3), \" \", starred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af2d19-30e9-4223-ad76-5235822767dc",
   "metadata": {},
   "source": [
    "## Fitted dimensions\n",
    "\n",
    "Schockaert and colleagues have a series of papers in which they consider interpretable dimensions in space from a knowledge base point of view. Their methods are completely different from the seed-based approach popular in NLP. We adapt an idea of Jameel and Schockaert to fit a dimension in space to best match human ratings.\n",
    "\n",
    "For every category/feature pair from Grand et al, we obtain a dimension that is a perfect fit to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f871d-986e-450c-8022-0d9c1672da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [ ]\n",
    "\n",
    "for grandcategory, grandfeature, pos_seedwords, neg_seedwords, df, data_vectors in each_grandcondition(grandratings_dir, grandfeatures_df):\n",
    "    \n",
    "    dimension = compute_dim.dimension_seedbased(pos_seedwords, neg_seedwords, word_vectors)\n",
    "    \n",
    "    # seed-based dimension\n",
    "    df[\"SPred\"]  = compute_dim.predict_scalarproj(data_vectors, dimension)\n",
    "    sresult_obj = stats.pearsonr(df[\"Gold\"], df[\"SPred\"])\n",
    "    \n",
    "    # fitted dimension\n",
    "    dimension, weight, bias = compute_dim.dimension_fitted_fromratings(data_vectors, df[\"Gold\"], feature_dim)\n",
    "    df[\"FPred\"] = compute_dim.predict_coord_fromline(data_vectors, dimension, weight, bias)\n",
    "    fresult_obj = stats.pearsonr(df[\"Gold\"], df[\"FPred\"])\n",
    "    \n",
    "    results.append({\"category\": grandcategory,\n",
    "                    \"feature\" : grandfeature,\n",
    "                    \"s_pearsonr\" : sresult_obj.statistic,\n",
    "                    \"s_pvalue\" : sresult_obj.pvalue,\n",
    "                    \"f_pearsonr\" : fresult_obj.statistic,\n",
    "                    \"f_pvalue\" : fresult_obj.pvalue } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176a874-d234-4ddb-b551-d45ee454359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of conditions with significant correlations:\")\n",
    "print(\"Seed-based:\", round(len([ r for r in results if r[\"s_pearsonr\"] > 0 and r[\"s_pvalue\"] < 0.05]) / len(results), 3))\n",
    "print(\"Fitted:\", round(len([ r for r in results if r[\"f_pearsonr\"] > 0 and r[\"f_pvalue\"] < 0.05]) / len(results), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113a5d5-2b38-440b-a8b4-204bc0801b3c",
   "metadata": {},
   "source": [
    "### Underdetermined dimensions\n",
    "\n",
    "However, the embeddings give us too much leeway to fit dimensions in space. Even when we scramble the ratings, the model mostly still manages to fit a dimension perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aba488-8824-4aa4-83d9-bfb73b5df6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "results = [ ]\n",
    "\n",
    "for grandcategory, grandfeature, pos_seedwords, neg_seedwords, df, data_vectors in each_grandcondition(grandratings_dir, grandfeatures_df):\n",
    "    \n",
    "    data_gold = [row.Gold for row in df.itertuples()]\n",
    "    random.shuffle(data_gold)\n",
    "    \n",
    "    # fitted dimension\n",
    "    dimension, weight, bias = compute_dim.dimension_fitted_fromratings(data_vectors, data_gold, feature_dim)\n",
    "    df[\"Pred\"] = compute_dim.predict_coord_fromline(data_vectors, dimension, weight, bias)\n",
    "    result_obj = stats.pearsonr(data_gold, df[\"Pred\"])\n",
    "    \n",
    "    results.append({\"category\": grandcategory,\n",
    "                    \"feature\" : grandfeature,\n",
    "                    \"pearsonr\" : result_obj.statistic,\n",
    "                    \"pvalue\" : result_obj.pvalue } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da253742-eeef-4d96-b850-e00c38341663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scrambled ratings: percentage of dimensions that could be fitted successfully\")\n",
    "print(round(len([ r for r in results if r[\"pearsonr\"] > 0 and r[\"pvalue\"] < 0.05]) / len(results), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17f8e9-221d-4aa6-963c-b47ee45bd6fe",
   "metadata": {},
   "source": [
    "That also means that the model overfits to the given data, and doesn't generalize well to new datapoints, as we will show below.\n",
    "\n",
    "To mitigate this problem, we combine the fitted model with seed property words, and we will be able to show that this leads to improved dimensions in space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9375c66-1c5a-49d8-93d6-0b16c47224a0",
   "metadata": {},
   "source": [
    "# Variants of the fitted model\n",
    "\n",
    "Seeds as words, match to seed-based dimensions as part of the loss, and both of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce8735-678c-4b14-ab8d-c7442ea3fe10",
   "metadata": {},
   "source": [
    "# Evaluating on unseen data\n",
    "\n",
    "We introduce a train/test split, or rather crossvalidation, to test how well different models do on unseen data.  But when we do that, we cannot use Pearson's r anymore: When there are few datapoints in the dataset, significance computation becomes unreliable.\n",
    "\n",
    "Instead, we will focus on (a variant of) OC_P, and we add mean square error to the picture. OC_P is highly correlated with Pearson's r; MSE less so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231df84-6e11-449b-a35f-13b0f66ceae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [ ]\n",
    "import compute_dim\n",
    "import eval_dim\n",
    "\n",
    "for grandcategory, grandfeature, pos_seedwords, neg_seedwords, df, data_vectors in each_grandcondition(grandratings_dir, grandfeatures_df):\n",
    "    \n",
    "    dimension = compute_dim.dimension_seedbased(pos_seedwords, neg_seedwords, word_vectors)\n",
    "    \n",
    "    df[\"Pred\"]  = compute_dim.predict_scalarproj(data_vectors, dimension)\n",
    "    \n",
    "    ocp = eval_dim.pairwise_order_consistency(df[\"Gold\"], df[\"Pred\"])\n",
    "    mse = eval_dim.mean_squared_error(df[\"Gold\"], df[\"Pred\"])\n",
    "    result_obj = stats.pearsonr(df[\"Gold\"], df[\"Pred\"])\n",
    "    \n",
    "    results.append({\"category\": grandcategory,\n",
    "                    \"feature\" : grandfeature,\n",
    "                    \"ocp\" : ocp,\n",
    "                    \"mse\" : mse,\n",
    "                    \"pearsonr\" : result_obj.statistic,\n",
    "                    \"pvalue\" : result_obj.pvalue } )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9585d-5f93-426d-a294-da2f76b16c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "result_obj = stats.pearsonr([r[\"pearsonr\"] for r in results], [r[\"ocp\"] for r in results])\n",
    "print(\"Correlation of Pearson's r and OC_P:\", result_obj.statistic, \"p=\", result_obj.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8bdba-e534-4e2e-9b80-f940999c093a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_obj = stats.pearsonr([r[\"pearsonr\"] for r in results], [r[\"mse\"] for r in results])\n",
    "print(\"Correlation of Pearson's r and MSE:\", result_obj.statistic, \"p=\", result_obj.pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43966ba0-00d5-400e-985d-33bb9a0e379c",
   "metadata": {},
   "source": [
    "The variant of OC_P that we use doesn't just compare pairwise orderings of the datapoints in the test set, but also pairwise orderings of test datapoints compared to training datapoints: Do the test datapoints get inserted at the right point in the overall ordering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d7858-c9cc-44c9-be5b-880ad0c10e2d",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization: GLoVE\n",
    "\n",
    "We make a development set, and use it to set the hyperparameters: offset and jitter for the seeds-as-words, alpha and averaging for seeds-as-dimensions, all of the above for the joint model, alpha for the seed-dimension-attention model. \n",
    "\n",
    "See other notebook. We use:\n",
    "\n",
    "* Fitted model with seed words: offset of 1.0, no jitter.  no difference across values\n",
    "* Fitted model with seed dimensions: alpha = 0.02, with averaging over seeds. \n",
    "* Fitted model with seed words and seed dimensions: alpha = 0.05, with averaging, no jitter, offset 1.0 (only alpha was optimized). \n",
    "\n",
    "Note that error bars are large in all cases especially when computing offsets for the fitted model with seed words. Here the differences in performance are dwarfed by the difference in noise, and all offsets show more or less equal performance. \n",
    "\n",
    "For the fitted model with seed dimensions, lowest values of alpha are generally best. This is not the case for the fitted model with seed words and seed dimensions, where the best values of alpha are low but not tiny. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89756d23-fad5-419a-b5be-b3dcc8aa0b52",
   "metadata": {},
   "source": [
    "## Crossvalidation on all data except the development data: GLoVE\n",
    "\n",
    "See  notebook grand_eval. We obtain:\n",
    "\n",
    "Seed-based method: OC_P 0.640 (0.11) MSE mean 72895.928 (341047.67) MSE med 2718.132 (5767.16)\n",
    "\n",
    "Fitted method: OC_P 0.540 (0.03) MSE mean 3708.000 (7665.66) MSE med 113.222 (111.67)\n",
    "\n",
    "Fitted, with seed words, offset 1.0 jitter False\n",
    "OC_P  0.532 (0.03) MSE mean 3232046.027 (22209200.36) MSE med 177.064 (125.35)\n",
    "\n",
    "Fitted, with seed dim.s, alpha 0.02 avg True\n",
    "OC_P  0.648 (0.11) MSE mean 11096.483 (55333.03) MSE med 89.607 (199.54)\n",
    "\n",
    "Fitted, with seed words and dim.s, alpha 0.05 avg True offset 1.0 jitter False\n",
    "OC_P  0.799 (0.06) MSE mean 0.707 (0.38) MSE med 0.666 (0.36)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef45df2-2df6-4446-977f-b7c15489c40b",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization: BERT large last 4 \n",
    "\n",
    "We make a development set, and use it to set the hyperparameters: offset and jitter for the seeds-as-words, alpha and averaging for seeds-as-dimensions, all of the above for the joint model, alpha for the seed-dimension-attention model. \n",
    "\n",
    "See other notebook. We use:\n",
    "\n",
    "* Fitted model with seed words: offset of 1.0, no jitter\n",
    "* Fitted model with seed dimensions: alpha = 0.001, with averaging\n",
    "* Fitted model with seed words and seed dimensions: alpha = 0.02. Values not optimized: averaging, no jitter, and an offset of 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5b7fc-d36b-4e68-87d0-28de460d7499",
   "metadata": {},
   "source": [
    "## Crossvalidation on all data except the development data: BERT-large last 4 \n",
    "\n",
    "Seed-based method: OC_P 0.637 (0.09) MSE mean 18430992.301 (130081284.87) MSE med 16892.690 (55100.49)\n",
    "\n",
    "Fitted method: OC_P 0.514 (0.03) MSE mean 86428.112 (358043.67) MSE med 417.379 (271.65)\n",
    "\n",
    "Fitted, with seed words, offset 1.0 jitter False\n",
    "OC_P  0.518 (0.03) MSE mean 92120.435 (454321.63) MSE med 597.424 (525.60)\n",
    "\n",
    "Fitted, with seed dim.s, alpha 0.001 avg True\n",
    "OC_P  0.656 (0.09) MSE mean 3456898.835 (24405540.37) MSE med 114.990 (437.41)\n",
    "\n",
    "Fitted, with seed words and dim.s, alpha 0.02 avg True offset 1.0 jitter False\n",
    "OC_P  0.705 (0.04) MSE mean 2.335 (0.70) MSE med 2.038 (0.59)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47eff2-6734-4f01-a997-400358957ad0",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization: RoBERTA large last 4 \n",
    "\n",
    "We make a development set, and use it to set the hyperparameters: offset and jitter for the seeds-as-words, alpha and averaging for seeds-as-dimensions, all of the above for the joint model, alpha for the seed-dimension-attention model. \n",
    "\n",
    "See other notebook. We use:\n",
    "\n",
    "* Fitted model with seed words: offset of 1.0, no jitter\n",
    "* Fitted model with seed dimensions: alpha = 0.001, with averaging\n",
    "* Fitted model with seed words and seed dimensions: alpha = 0.02. Values not optimized: averaging, no jitter, and an offset of 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4639a3-3c4f-447f-9e61-d64f5fa5fabd",
   "metadata": {},
   "source": [
    "## Crossvalidation on all data except the development data: RoBERTA-large last 4 \n",
    "\n",
    "Seed-based method: OC_P 0.572 (0.08) MSE mean 548231.690 (3496790.12) MSE med 25783.635 (95616.32)\n",
    "\n",
    "Fitted method: OC_P 0.507 (0.03) MSE mean 6963633.529 (47257020.62) MSE med 392.507 (291.33)\n",
    "\n",
    "Fitted, with seed words, offset 1.0 jitter False\n",
    "OC_P  0.506 (0.03) MSE mean 32917.013 (131398.84) MSE med 458.028 (284.34)\n",
    "\n",
    "Fitted, with seed dim.s, alpha 0.001 avg True\n",
    "OC_P  0.598 (0.09) MSE mean 10022.875 (54560.49) MSE med 125.192 (270.52)\n",
    "\n",
    "Fitted, with seed words and dim.s, alpha 0.02 avg True offset 1.0 jitter False\n",
    "OC_P  0.688 (0.04) MSE mean 2.196 (0.64) MSE med 1.922 (0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeda4d4-643d-44e5-b8ad-25721fae909f",
   "metadata": {},
   "source": [
    "# Correlation and pairwise order consistency\n",
    "\n",
    "Testing on Grand data, with GLoVE: Are correlation and OC_P correlated? How about correlation and our new pairwise rank and insertion accuracy? \n",
    "\n",
    "We evaluate with correlation and normal OC_P on the whole dataset, and do 5-fold crossvalidation for rank and insertion accuracy. \n",
    "\n",
    "We obtain:\n",
    "```\n",
    "correlation of Pearson's r and r-acc on seed-based dimensions\n",
    "Grand data\n",
    "Correlation r = 0.972 p = 0.000\n",
    "\n",
    "correlation of Pearson's r and ri-acc on seed-based dimensions\n",
    "Grand data\n",
    "Correlation r = 0.971 p = 0.000\n",
    "```\n",
    "\n",
    "It is not surprising that the outcomes are basically the same because in this setting ri-acc is the same as r-acc just with a few rank comparisons omitted. \n",
    "\n",
    "Still, it needed to be done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58424e1-aa76-41d3-9191-0887f5bbd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import compute_dim\n",
    "import eval_dim\n",
    "from scipy import stats\n",
    "import statistics\n",
    "\n",
    "results = [ ]\n",
    "\n",
    "rng = np.random.default_rng(seed = 3)\n",
    "\n",
    "\n",
    "for grandcategory, grandfeature, pos_seedwords, neg_seedwords, df, data_vectors in each_grandcondition(grandratings_dir, grandfeatures_df):\n",
    "    \n",
    "    dimension = compute_dim.dimension_seedbased(pos_seedwords, neg_seedwords, word_vectors)\n",
    "    \n",
    "    df[\"Pred\"]  = compute_dim.predict_scalarproj(data_vectors, dimension)\n",
    "    \n",
    "    ocp = eval_dim.pairwise_order_consistency(df[\"Gold\"], df[\"Pred\"])\n",
    "    result_obj = stats.pearsonr(df[\"Gold\"], df[\"Pred\"])\n",
    "    \n",
    "    # use crossvalidation to compute rank & insertion accuracies, and average\n",
    "    # over them\n",
    "    ri_accs = [ ]\n",
    "    # crossvalidation setup: give indices to datapoints\n",
    "    fold = rng.integers(low = 0, high = 5, size = len(df.Pred))\n",
    "    \n",
    "    for testfold in range(5):\n",
    "        # compute training and test data for this fold\n",
    "        test_indices =  [i for i in range(len(df.Pred)) if fold[i] == testfold]\n",
    "\n",
    "        ri_accs.append( eval_dim.pairwise_order_consistency_wrt(df.Gold, df.Pred, test_indices))\n",
    "                       \n",
    "    \n",
    "    results.append({\"category\": grandcategory,\n",
    "                    \"feature\" : grandfeature,\n",
    "                    \"ocp\" : ocp,\n",
    "                    \"ri_acc_avg\" : statistics.mean(ri_accs),\n",
    "                    \"pearsonr\" : result_obj.statistic,\n",
    "                    \"pvalue\" : result_obj.pvalue } )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44095e2-6199-4d80-82b5-5caa5f73890f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "print(\"correlation of Pearson's r and r-acc on seed-based dimensions\")\n",
    "print(\"Grand data\")\n",
    "result_obj = stats.pearsonr([r[\"pearsonr\"] for r in results],\n",
    "                            [r[\"ocp\"] for r in results])\n",
    "print(f\"Correlation r = {result_obj.statistic:.3f} p = {result_obj.pvalue:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"correlation of Pearson's r and ri-acc on seed-based dimensions\")\n",
    "print(\"Grand data\")\n",
    "result_obj = stats.pearsonr([r[\"pearsonr\"] for r in results],\n",
    "                            [r[\"ri_acc_avg\"] for r in results])\n",
    "print(f\"Correlation r = {result_obj.statistic:.3f} p = {result_obj.pvalue:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7897fc8-c015-4ae9-88e6-ee066daa89bb",
   "metadata": {},
   "source": [
    "# Inspecting MSE\n",
    "\n",
    "Mean MSE values are really high for almost all models, but medians generally look much better, which means that there are likely some crazy outliers. We inspect this in more detail. \n",
    "\n",
    "To do this, we first rerun the whole evaluation, all models, all crossvalidation runs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc8a61-e320-4e00-bebc-db0d5fe0e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_dim\n",
    "import compute_dim\n",
    "import statistics\n",
    "\n",
    "def crossvalidation(filenames, method, word_vectors, grandratings_dir, grandfeatures_df, random_seed = 123, verbose = False):\n",
    "    \n",
    "    all_evals = [ ]\n",
    "    rng = np.random.default_rng(seed = 3)\n",
    "\n",
    "    \n",
    "    for filename in filenames:\n",
    "            grandcategory, grandfeature, pos_seedwords, neg_seedwords, df = read_grand_data(filename, \n",
    "                                                                                            grandratings_dir,                                                                                            grandfeatures_df)\n",
    "            # storage for word vectors and gold values for this dataset\n",
    "            all_thisdata_vectors = []\n",
    "            all_thisdata_gold = []\n",
    "\n",
    "            # collect word vectors and gold ratings\n",
    "            for row in df.itertuples():\n",
    "                # row.Row is the word. look it up in word_vectors\n",
    "                all_thisdata_vectors.append( word_vectors[ row.Row ])\n",
    "                # gold rating: use z-scored average\n",
    "                all_thisdata_gold.append( row.Gold)\n",
    "\n",
    "            # crossvalidation setup: give indices to datapoints\n",
    "            fold = rng.integers(low = 0, high = method[\"numfolds\"], size = len(all_thisdata_gold))\n",
    "\n",
    "            # store the evaluation results from the different test folds\n",
    "            evals = [ ]\n",
    "\n",
    "            # iterate over folds, evaluate for each of them\n",
    "            for testfold in range(method[\"numfolds\"]):\n",
    "                # compute training and test data for this fold\n",
    "                test_indices =  [i for i in range(len(all_thisdata_gold)) if fold[i] == testfold]\n",
    "                train_indices = [i for i in range(len(all_thisdata_gold)) if fold[i] != testfold]\n",
    "\n",
    "                gold_test =  [ell[\"Gold\"] for _, ell in df.iloc[ test_indices ].iterrows()]\n",
    "                gold_train = [ ell[\"Gold\"] for _, ell in df.iloc[ train_indices ].iterrows()]\n",
    "                words_test =  [ell[\"Row\"] for _, ell in df.iloc[ test_indices].iterrows()]\n",
    "                words_train = [ell[\"Row\"] for _, ell in df.iloc[ train_indices].iterrows()]\n",
    "                vec_test =  [word_vectors[ w ] for w in words_test]\n",
    "                vec_train = [word_vectors[ w ] for w in words_train ]\n",
    "\n",
    "\n",
    "                # compute seed-based dimension, and its predictions\n",
    "                if method[\"method\"] == \"seedbased\":\n",
    "                    dimension = compute_dim.dimension_seedbased(pos_seedwords, neg_seedwords, word_vectors)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromtrain(vec_train, gold_train, dimension, all_thisdata_vectors)\n",
    "\n",
    "                elif method[\"method\"] == \"fitted\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings(vec_train, gold_train, \n",
    "                                                                                       method[\"feature_dim\"],\n",
    "                                                                                       random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                elif method[\"method\"] == \"fitted_seedwords\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings_seedwords(vec_train, gold_train, \n",
    "                                                                    method[\"feature_dim\"], \n",
    "                                                                    pos_seedwords, neg_seedwords, word_vectors,\n",
    "                                                                    offset = method[\"offset\"], jitter = method[\"jitter\"],\n",
    "                                                                    random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                elif method[\"method\"] == \"fitted_seeddims\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings_seeddims(vec_train, gold_train, \n",
    "                                                                    method[\"feature_dim\"], \n",
    "                                                                    pos_seedwords, neg_seedwords, word_vectors,\n",
    "                                                                    do_average = method[\"do_average\"], \n",
    "                                                                    alpha = method[\"alpha\"],\n",
    "                                                                    random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                elif method[\"method\"] == \"combined\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings_combined(vec_train, gold_train,\n",
    "                                                                    method[\"feature_dim\"],\n",
    "                                                                    pos_seedwords, neg_seedwords, word_vectors,\n",
    "                                                                    offset = method[\"offset\"], jitter = method[\"jitter\"],\n",
    "                                                                    do_average = method[\"do_average\"], \n",
    "                                                                    alpha = method[\"alpha\"],\n",
    "                                                                    random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"shouldn't be here\")\n",
    "\n",
    "                # order consistency pairwise: test values tested for their ordering wrt. all values, training and test\n",
    "                # MSE: evaluate on test only\n",
    "                e = { \"ocp\" : eval_dim.pairwise_order_consistency_wrt(df[\"Gold\"], df[\"Pred\"], test_indices),\n",
    "                      \"mse\" : eval_dim.mean_squared_error(gold_test, [p for i, p in enumerate(df[\"Pred\"]) if i in test_indices]),\n",
    "                      \"feature\" : grandfeature,\n",
    "                      \"category\" : grandcategory,\n",
    "                      \"method\" : method[\"method\"]}\n",
    "\n",
    "                all_evals.append(e)\n",
    "\n",
    "    if verbose:\n",
    "        ocps = [e[\"ocp\"] for e in all_evals if e[\"ocp\"] is not None]\n",
    "        mses = [e[\"mse\"] for e in all_evals if e[\"mse\"] is not None]\n",
    "\n",
    "        print(\"\\n\\nOverall\", method[\"method\"], \n",
    "              f\"OC_p {statistics.mean(ocps):.3f} ({statistics.stdev(ocps):.2f})\", \n",
    "              f\"MSE mean {statistics.mean(mses):.3f} ({statistics.stdev(mses):.2f}) median {statistics.median(mses):.3f}\")\n",
    "        \n",
    "    return all_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b6b76-2cdb-419b-9586-d8af3d7117d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# non-development data\n",
    "\n",
    "filenames = [f for f in os.listdir(grandratings_dir) if f.endswith(\"csv\")]\n",
    "\n",
    "import random\n",
    "random.seed(789)\n",
    "devset = random.sample(filenames, 6)\n",
    "traintestset = [f for f in filenames if f not in devset]\n",
    "[ filename[:-4].split(\"_\") for filename in traintestset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bfb1e-9b33-4995-8286-11968bb446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for the different runs\n",
    "\n",
    "numfolds = 5\n",
    "feature_dim = 300\n",
    "num_randseeds = 3\n",
    "\n",
    "hyper_offset = 1.0\n",
    "hyper_jitter = False\n",
    "hyper_average = True\n",
    "hyper_alpha1 = 0.02\n",
    "hyper_alpha2 = 0.05\n",
    "\n",
    "run_settings = { \"seedbased\": { \"method\": \"seedbased\",\n",
    "                                \"numfolds\" : numfolds},\n",
    "                \"fitted\" :    {\"method\": \"fitted\",\n",
    "                                \"numfolds\" : numfolds,\n",
    "                                \"feature_dim\" : feature_dim},\n",
    "                \"fitted_seedwords\" : { \"method\": \"fitted_seedwords\",\n",
    "                                \"numfolds\" : numfolds,\n",
    "                                \"offset\" : hyper_offset,\n",
    "                                \"jitter\" : hyper_jitter,\n",
    "                                \"feature_dim\" : feature_dim},\n",
    "                \"fitted_seeddims\" : { \"method\": \"fitted_seeddims\",\n",
    "                                \"numfolds\" : numfolds,\n",
    "                                \"alpha\" : hyper_alpha1,\n",
    "                                \"do_average\" : hyper_average,\n",
    "                                \"feature_dim\" : feature_dim},\n",
    "                \"combined\" : {\"method\": \"combined\",\n",
    "                                \"numfolds\" : numfolds,\n",
    "                                \"alpha\" : hyper_alpha2,\n",
    "                                \"do_average\" : hyper_average,\n",
    "                                \"offset\" : hyper_offset,\n",
    "                                \"jitter\" : hyper_jitter,\n",
    "                                \"feature_dim\" : feature_dim}\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4391d-eca6-405d-92f3-7c45c09838c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = [ ]\n",
    "for methodname in run_settings.keys():\n",
    "    print(methodname)\n",
    "    \n",
    "    random.seed(5)\n",
    "    randoms = [random.randrange(0,100) for _ in range(num_randseeds)]\n",
    "\n",
    "    for rval in randoms:\n",
    "        theseresults = crossvalidation(traintestset, run_settings[methodname], \n",
    "                                       word_vectors, grandratings_dir, grandfeatures_df, random_seed = rval)\n",
    "        results += theseresults\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5d03b-e356-4ade-9f21-ea40c94b8fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a3034-095b-4e8a-b118-a66f635254bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22d9dc-25e3-4e1a-8e5d-dc81cf9b2ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counting how often we get MSE values in different bins\n",
    "# as I can't seem to make pandas barplot to do the right thing\n",
    "\n",
    "counts = [ ]\n",
    "namemap= {\"seedbased\" : \"seed\",\n",
    "          \"fitted\" : \"fit\",\n",
    "          \"fitted_seedwords\" : \"fit-sw\",\n",
    "          \"fitted_seeddims\" : \"fit-sd\",\n",
    "          \"combined\" : \"fit-s\" }\n",
    "\n",
    "for method in [\"seedbased\", \"fitted\", \"fitted_seedwords\", \"fitted_seeddims\", \"combined\"]:\n",
    "    counts.append( { \"model\" : namemap[method],\n",
    "                    \"<2\": res_df[(res_df.method == method) & (res_df.mse < 2)].shape[0],\n",
    "                    \"2-10\": res_df[(res_df.method == method) & (res_df.mse>= 2) & (res_df.mse < 10)].shape[0],\n",
    "                    \"10-100\" : res_df[(res_df.method == method) & (res_df.mse >= 10) & (res_df.mse < 100)].shape[0],\n",
    "                    \">100\":res_df[(res_df.method == method) & (res_df.mse >= 100)].shape[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcaea9-f00c-4450-b28a-6cb47605f537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countsdf = pd.DataFrame(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7bc5bb-d596-4f45-a8c3-bfefe9f16c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countsdf.set_index(\"model\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a8895-deae-4028-9964-bf017fc459d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "countsdf.plot.bar(rot=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb45f80-c353-4c80-bc3f-aba61ed6f34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for method in [\"seedbased\", \"fitted\", \"fitted_seedwords\", \"fitted_seeddims\", \"combined\"]:\n",
    "    numentries = res_df[res_df.method == method].shape[0]\n",
    "    lt2 = res_df[(res_df.method == method) & (res_df.mse < 2)].shape[0]\n",
    "    lt10 = res_df[(res_df.method == method) & (res_df.mse < 10)].shape[0]\n",
    "    print(\"Model\", method, \"percentage with MSE<2:\", lt2, \"of\", numentries, \"=\", round(lt2 / numentries, 4))\n",
    "    print(\"\\t\", \"percentage with MSE<10:\", lt10, \"of\", numentries, \"=\", round(lt10 / numentries, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9c169-cadd-4f58-bd96-6543adf21fd5",
   "metadata": {},
   "source": [
    "# Where is the difference?\n",
    "\n",
    "On the Grand et al object properties, the fitted model with seeds as words and dimensions does considerably better than seed-based dimensions. But when does it do better? Does it do better specifically on cases where the original seed-based dimensions did not do well? Is there any regularity to when this model is preferable? e inspect the \"seed-based\" and \"combined\" results among the results computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1119014-30a9-4258-9b47-42dc49f7f705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select results dictionaries for seedbased and combined methods\n",
    "results_seed = [r for r in results if r[\"method\"] == \"seedbased\"]\n",
    "results_comb = [r for r in results if r[\"method\"] == \"combined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf249d3-89f0-4787-87e9-0454f3776af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "# given a list of results dictionaries, \n",
    "# group them by the given dictionary keys\n",
    "# return as a dictionary keys -> results dictionaries\n",
    "def eval_aggregate_by(evals, keylabels):\n",
    "    bydataset_eval = defaultdict(list)\n",
    "    \n",
    "    for e in evals:\n",
    "        key = tuple([str(e[k]) for k in keylabels])\n",
    "        bydataset_eval[ key ].append(e)\n",
    "        \n",
    "    return bydataset_eval\n",
    "\n",
    "# given a list of results dictionaries,\n",
    "# compute mean, median and standard deviation over values for a particular key\n",
    "def eval_summary_by(evals, keylabel):\n",
    "    vals = [e[keylabel] for e in evals if e[keylabel] is not None]\n",
    "    \n",
    "    return (statistics.mean(vals), statistics.median(vals), statistics.stdev(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472860b-3c41-4c22-ba1a-16adbb6f2106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for seedbased and combined,\n",
    "# aggregate results by condition,\n",
    "# yielding a dictionary mapping (category, feature) to evaluation results\n",
    "results_seed_bycond = eval_aggregate_by(results_seed, [\"category\", \"feature\"])\n",
    "results_comb_bycond = eval_aggregate_by(results_comb, [\"category\", \"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf52e3-ceb8-4e52-b886-ad1eadb5cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into a list of dictionaries, one per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2091c5-0db7-41f1-babe-7058a4d50562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_and_comb_list = [ ]\n",
    "for condition in results_seed_bycond.keys():\n",
    "    category, feature = condition\n",
    "    \n",
    "    # keep mean OCP, median MSE only\n",
    "    rs = results_seed_bycond[condition]\n",
    "    ocp_seed = eval_summary_by(rs, \"ocp\")[0]\n",
    "    mse_seed = eval_summary_by(rs, \"mse\")[1]\n",
    "                               \n",
    "    rs = results_comb_bycond[condition]\n",
    "    ocp_comb = eval_summary_by(rs, \"ocp\")[0]\n",
    "    mse_comb = eval_summary_by(rs, \"mse\")[1]\n",
    "    \n",
    "    seed_and_comb_list.append({ \"category\":category, \"feature\":feature,\n",
    "                                \"ocp_seed\" : ocp_seed, \"mse_seed\" : mse_seed,\n",
    "                                \"ocp_comb\" : ocp_comb, \"mse_comb\" : mse_comb})\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3c3f2-33bc-486c-aa6e-b2feccb02458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(seed_and_comb_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102ba8a-2240-4fda-bdd7-8a7be774f5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in what percentage of cases did ocp get better?\n",
    "print(\"How often did OCP rise from seed-based to fit-s?\")\n",
    "print(df[df.ocp_comb > df.ocp_seed].shape[0], \"out of\", df.shape[0],\n",
    "      \" -- percentage:\", round(df[df.ocp_comb > df.ocp_seed].shape[0] / df.shape[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dd4d3-2d30-49c6-b71b-022291c0a21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how strongly did ocp rise for different bins of\n",
    "# ocp in seed-based?\n",
    "# It does indeed rise the most for the cases where the seed-based results were weakest\n",
    "\n",
    "# rise in ocp\n",
    "\n",
    "df[\"ocp_rise\"] = df.ocp_comb - df.ocp_seed\n",
    "\n",
    "df[\"ocp_seed_bin\"] = pd.cut(df.ocp_seed, 5)\n",
    "df.groupby(by = \"ocp_seed_bin\").ocp_rise.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648b7e1-2e3d-4f59-b927-971092dfd14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby(by = \"ocp_seed_bin\").ocp_rise.mean().plot.bar(rot=30 ,xlabel=\"seed-based ri-acc\", ylabel=\"fit+s ri-acc increase\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eabf5b-5591-4f2d-8adc-0e3200a1f0b4",
   "metadata": {},
   "source": [
    "# Printing out all the results in a table, to choose from for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6a96c-f8da-40a2-92ae-6a09cf55aecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine all results into one dictionary per condition\n",
    "\n",
    "results_seed_bycond = eval_aggregate_by([r for r in results if r[\"method\"] == \"seedbased\"], [\"category\", \"feature\"])\n",
    "results_fit_bycond = eval_aggregate_by([r for r in results if r[\"method\"] == \"fitted\"], [\"category\", \"feature\"])\n",
    "results_fitsw_bycond = eval_aggregate_by([r for r in results if r[\"method\"] == \"fitted_seedwords\"], [\"category\", \"feature\"])\n",
    "results_fitsd_bycond = eval_aggregate_by([r for r in results if r[\"method\"] == \"fitted_seeddims\"], [\"category\", \"feature\"])\n",
    "results_fits_bycond = eval_aggregate_by([r for r in results if r[\"method\"] == \"combined\"], [\"category\", \"feature\"])\n",
    "\n",
    "resultdict_list = [ ]\n",
    "for condition in results_seed_bycond.keys():\n",
    "    category, feature = condition\n",
    "    \n",
    "    e = { \"category\":category, \"feature\":feature, \"ocp\":{}, \"mse\" : {} }\n",
    "    \n",
    "    # keep mean OCP, median MSE only\n",
    "    rs = results_seed_bycond[condition]\n",
    "    e[\"ocp\"][\"seed\"] = eval_summary_by(rs, \"ocp\")[0]\n",
    "    e[\"mse\"][\"seed\"] = eval_summary_by(rs, \"mse\")[1]\n",
    "                               \n",
    "    rs = results_fit_bycond[condition]\n",
    "    e[\"ocp\"][\"fit\"] = eval_summary_by(rs, \"ocp\")[0]\n",
    "    e[\"mse\"][\"fit\"] = eval_summary_by(rs, \"mse\")[1]\n",
    "    \n",
    "    rs = results_fitsw_bycond[condition]\n",
    "    e[\"ocp\"][\"fit-sw\"] = eval_summary_by(rs, \"ocp\")[0]\n",
    "    e[\"mse\"][\"fit-sw\"] = eval_summary_by(rs, \"mse\")[1]\n",
    "    \n",
    "    rs = results_fitsd_bycond[condition]\n",
    "    e[\"ocp\"][\"fit-sd\"] = eval_summary_by(rs, \"ocp\")[0]\n",
    "    e[\"mse\"][\"fit-sd\"] = eval_summary_by(rs, \"mse\")[1]\n",
    "    \n",
    "    rs = results_fits_bycond[condition]\n",
    "    e[\"ocp\"][\"fit-s\"] = eval_summary_by(rs, \"ocp\")[0]\n",
    "    e[\"mse\"][\"fit-s\"] = eval_summary_by(rs, \"mse\")[1]\n",
    "    \n",
    "    resultdict_list.append(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8039d8-aeb0-4ef3-ba17-5ad64591031f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a LaTeX table (minus header)\n",
    "# category feature, ocp's, mse's\n",
    "# for results it's always first seed, then fit, then fit-sw, fit-sd, fit-s\n",
    "# for now, sort by seed ocp\n",
    "for e in sorted(resultdict_list, key = lambda d:d[\"ocp\"][\"seed\"]):\n",
    "    print(e[\"category\"], \"&\", e[\"feature\"], \"&\", end = \" \")\n",
    "    models = [\"seed\", \"fit\", \"fit-sw\", \"fit-sd\", \"fit-s\"]\n",
    "    for m in models:\n",
    "        print(round(e[\"ocp\"][m],2), end = \" \")\n",
    "        if m != \"fit-s\":\n",
    "            print(\"& \", end = \" \")\n",
    "    # for m in models:\n",
    "    #     print(round(e[\"mse\"][m], 2), \"&\", end = \" \")\n",
    "    print(\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b6e65-a0db-48f2-bd04-f109b1a31a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
