{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5480aa-faa7-4368-8975-6a0a1ea34abe",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization: Contextualized embeddings \n",
    "\n",
    "Grand et al data\n",
    "\n",
    "Hyperparameter optimization on a development set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327a05a-9d45-426e-b640-6748633897ae",
   "metadata": {},
   "source": [
    "# Global data, set to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444da84e-94ed-4d65-926c-c0915c69c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grandratings_dir = \"../data/Grandetal-data/\"\n",
    "grandfeatures_path = \"../data/Grandetal-data/features.xlsx\"\n",
    "\n",
    "# embpath = \"./\"\n",
    "embpath = \"../vectors/bert_vectors\"       \n",
    "# embpath = \"../vectors/roberta-large-vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7b586-6498-44fb-8810-5cbe92e383ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import stats\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import math\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf61834-8f0f-4580-88da-f4f85292ff7d",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "## Contextualized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6a243-ae47-49a7-88ce-1f6280d0b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "num_randseeds = 3\n",
    "feature_dim = 1024\n",
    "whichbert = \"robltop4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1e40a-6973-46e5-bc60-750089d396f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_large_vecs_paths = { \"ltop4\" : embpath + \"bert-large-uncased.top4layers.Grandetal.npz\",\n",
    "                          \"robltop4\" : embpath + \"roberta-large.Grandetal.top4layers.pkl\"}\n",
    "                         \n",
    "print(\"using contextualized embeddings\", bert_large_vecs_paths[whichbert], \"\\n\")\n",
    "\n",
    "filename = bert_large_vecs_paths[whichbert]\n",
    "if filename.endswith(\"npz\"):\n",
    "    # for npz files:\n",
    "    data = np.load(bert_large_vecs_paths[whichbert])\n",
    "elif filename.endswith(\"pkl\"):\n",
    "    # for pkl files:\n",
    "    with open(bert_large_vecs_paths[whichbert], \"rb\") as f:\n",
    "        data2 = pickle.load(f)\n",
    "        data = dict([(w, v.numpy()) for w, v in data2.items()])\n",
    "else:\n",
    "    raise Exception(\"should not be here\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c7af5-e421-4a67-9a4c-05ce8edb010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "    \n",
    "for word in data:\n",
    "    vector = data[word]\n",
    "    if ' ' in word:\n",
    "        first, second = word.split()\n",
    "        if 'north' in first and 'dakota' in second:\n",
    "            word = first + '_' + second\n",
    "            print(word)\n",
    "            word_vectors[word] = vector\n",
    "        else:\n",
    "            word = first + '-' + second\n",
    "            print(word)\n",
    "            word_vectors[word] = vector\n",
    "    else:\n",
    "        word_vectors[word] = vector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07f037-d314-409b-b0f8-c5d060d5e721",
   "metadata": {},
   "source": [
    "## Grand features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56398ff-3d19-4544-bc82-06a60e6d8737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "grandfeatures_df = pd.read_excel(grandfeatures_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8216d9-2d9d-43c0-989e-a173a3e4be84",
   "metadata": {},
   "source": [
    "## Function for reading a specific Grand dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45bb6e-da17-4581-91b2-41d3721ce40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading in Grand data\n",
    "def read_grand_data(filename, grandratings_dir, grandfeatures_df):\n",
    "    # extract category and feature\n",
    "    grandcategory, grandfeature = filename[:-4].split(\"_\")\n",
    "        \n",
    "    # read human ratings, make gold column\n",
    "    df = pd.read_csv(grandratings_dir + filename)\n",
    "    nspeakers = len(df.columns) -1\n",
    "    df[\"Average\"] = [row.iloc[1:26].sum() / nspeakers for _, row in df.iterrows()]\n",
    "    # z-scores of average ratings\n",
    "    df[\"Gold\"] = (df[\"Average\"] - df[\"Average\"].mean()) / df[\"Average\"].std()\n",
    "        \n",
    "    # obtain seed words from excel file\n",
    "    relevant_row = grandfeatures_df[grandfeatures_df.Dimension == grandfeature]\n",
    "    seedwords = relevant_row.iloc[:, 1:].values.flatten().tolist()\n",
    "    pos_seedwords = seedwords[:3]\n",
    "    neg_seedwords = seedwords[3:]\n",
    "    \n",
    "    return (grandcategory, grandfeature, pos_seedwords, neg_seedwords, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21844b50-6e40-455d-a97c-dff06b04b412",
   "metadata": {},
   "source": [
    "# Function for running crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfd9f4-3f6d-47f1-848d-188ede0b9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_dim\n",
    "import compute_dim\n",
    "import statistics\n",
    "\n",
    "def crossvalidation(filenames, method, word_vectors, grandratings_dir, grandfeatures_df, random_seed = 123, verbose = False):\n",
    "    \n",
    "    all_evals = [ ]\n",
    "    \n",
    "    for filename in filenames:\n",
    "            grandcategory, grandfeature, pos_seedwords, neg_seedwords, df = read_grand_data(filename, \n",
    "                                                                                            grandratings_dir, \n",
    "                                                                                            grandfeatures_df)\n",
    "\n",
    "\n",
    "            # storage for word vectors and gold values for this dataset\n",
    "            all_thisdata_vectors = []\n",
    "            all_thisdata_gold = []\n",
    "\n",
    "            # collect word vectors and gold ratings\n",
    "            for row in df.itertuples():\n",
    "                # row.Row is the word. look it up in word_vectors\n",
    "                all_thisdata_vectors.append( word_vectors[ row.Row ])\n",
    "                # gold rating: use z-scored average\n",
    "                all_thisdata_gold.append( row.Gold)\n",
    "\n",
    "            # crossvalidation setup: give indices to datapoints\n",
    "            fold = np.random.randint(method[\"numfolds\"], size = len(all_thisdata_gold))\n",
    "\n",
    "            # store the evaluation results from the different test folds\n",
    "            evals = [ ]\n",
    "\n",
    "            # iterate over folds, evaluate for each of them\n",
    "            for testfold in range(method[\"numfolds\"]):\n",
    "                # compute training and test data for this fold\n",
    "                test_indices =  [i for i in range(len(all_thisdata_gold)) if fold[i] == testfold]\n",
    "                train_indices = [i for i in range(len(all_thisdata_gold)) if fold[i] != testfold]\n",
    "\n",
    "                gold_test =  [ell[\"Gold\"] for _, ell in df.iloc[ test_indices ].iterrows()]\n",
    "                gold_train = [ ell[\"Gold\"] for _, ell in df.iloc[ train_indices ].iterrows()]\n",
    "                words_test =  [ell[\"Row\"] for _, ell in df.iloc[ test_indices].iterrows()]\n",
    "                words_train = [ell[\"Row\"] for _, ell in df.iloc[ train_indices].iterrows()]\n",
    "                vec_test =  [word_vectors[ w ] for w in words_test]\n",
    "                vec_train = [word_vectors[ w ] for w in words_train ]\n",
    "\n",
    "\n",
    "                # compute seed-based dimension, and its predictions\n",
    "                if method[\"method\"] == \"seedbased\":\n",
    "                    dimension = compute_dim.dimension_seedbased(pos_seedwords, neg_seedwords, word_vectors)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromtrain(vec_train, gold_train, dimension, all_thisdata_vectors)\n",
    "\n",
    "                elif method[\"method\"] == \"fitted\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings(vec_train, gold_train, \n",
    "                                                                                       method[\"feature_dim\"],\n",
    "                                                                                       random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                elif method[\"method\"] == \"fitted_seedwords\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings_seedwords(vec_train, gold_train, \n",
    "                                                                    method[\"feature_dim\"], \n",
    "                                                                    pos_seedwords, neg_seedwords, word_vectors,\n",
    "                                                                    offset = method[\"offset\"], jitter = method[\"jitter\"],\n",
    "                                                                    random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                elif method[\"method\"] == \"fitted_seeddims\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings_seeddims(vec_train, gold_train, \n",
    "                                                                    method[\"feature_dim\"], \n",
    "                                                                    pos_seedwords, neg_seedwords, word_vectors,\n",
    "                                                                    do_average = method[\"do_average\"], \n",
    "                                                                    alpha = method[\"alpha\"],\n",
    "                                                                    random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                elif method[\"method\"] == \"combined\":\n",
    "                    dimension, weight, bias = compute_dim.dimension_fitted_fromratings_combined(vec_train, gold_train,\n",
    "                                                                    method[\"feature_dim\"],\n",
    "                                                                    pos_seedwords, neg_seedwords, word_vectors,\n",
    "                                                                    offset = method[\"offset\"], jitter = method[\"jitter\"],\n",
    "                                                                    do_average = method[\"do_average\"], \n",
    "                                                                    alpha = method[\"alpha\"],\n",
    "                                                                    random_seed = random_seed)\n",
    "                    df[\"Pred\"] = compute_dim.predict_coord_fromline(all_thisdata_vectors, dimension, weight, bias)\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"shouldn't be here\")\n",
    "\n",
    "                # order consistency pairwise: test values tested for their ordering wrt. all values, training and test\n",
    "                # MSE: evaluate on test only\n",
    "                e = { \"ocp\" : eval_dim.pairwise_order_consistency_wrt(df[\"Gold\"], df[\"Pred\"], test_indices),\n",
    "                      \"mse\" : eval_dim.mean_squared_error(gold_test, [p for i, p in enumerate(df[\"Pred\"]) if i in test_indices]),\n",
    "                      \"feature\" : grandfeature,\n",
    "                      \"category\" : grandcategory}\n",
    "\n",
    "                all_evals.append(e)\n",
    "\n",
    "    if verbose:\n",
    "        ocps = [e[\"ocp\"] for e in all_evals if e[\"ocp\"] is not None]\n",
    "        mses = [e[\"mse\"] for e in all_evals if e[\"mse\"] is not None]\n",
    "\n",
    "        print(\"\\n\\nOverall\", method[\"method\"], \n",
    "              f\"OC_p {statistics.mean(ocps):.3f} ({statistics.stdev(ocps):.2f})\", \n",
    "              f\"MSE mean {statistics.mean(mses):.3f} ({statistics.stdev(mses):.2f}) median {statistics.median(mses):.3f}\")\n",
    "        \n",
    "    return all_evals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92cc5e-b162-44d1-82b7-ffca22d67a7f",
   "metadata": {},
   "source": [
    "# Function for aggregating crossvalidation results\n",
    "\n",
    "We assume results that are dictionaries. \n",
    "\n",
    "First, a function for aggregating results by particular labels, into a single dictionary. For example, when results are aggregated by category and feature, the result will be a dictionary whose keys are category/feature tuples, and the values are lists of result dictionaries. \n",
    "\n",
    "Second, a function that computes mean, median, and standard deviation, over a list of result dictionaries, for a given labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35093491-406a-4724-b31f-88940d4ee982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "# given a list of results dictionaries, \n",
    "# group them by the given dictionary keys\n",
    "def eval_aggregate_by(evals, keylabels):\n",
    "    bydataset_eval = defaultdict(list)\n",
    "    \n",
    "    for e in evals:\n",
    "        key = tuple([str(e[k]) for k in keylabels])\n",
    "        bydataset_eval[ key ].append(e)\n",
    "        \n",
    "    return bydataset_eval\n",
    "\n",
    "\n",
    "# given a list of results dictionaries,\n",
    "# compute mean, median and standard deviation over values for a particular key\n",
    "def eval_summary_by(evals, keylabel):\n",
    "    vals = [e[keylabel] for e in evals if e[keylabel] is not None]\n",
    "    \n",
    "    return (statistics.mean(vals), statistics.median(vals), statistics.stdev(vals))\n",
    "\n",
    "# given a dictionary of results (parameters -> result dictionary list),\n",
    "# * for each parameter setting, aggregate by cateogy and feature\n",
    "# * for each category/feature, compute mean ocp and mse values\n",
    "# * for the parameter setting, compute mean and sd ocp and mse over all category/feature pairs\n",
    "def eval_hyper(results, parameternames):\n",
    "    # output dictionary\n",
    "    results_byparam = [ ]\n",
    "\n",
    "    # iterate over parameter settings\n",
    "    for theseresults in results.values():\n",
    "        # extract parameters\n",
    "        this_dict = dict([(par, theseresults[0][par]) for par in parameternames])\n",
    "        \n",
    "        # aggregate by condition = by category and feature\n",
    "        results_bycond = eval_aggregate_by(theseresults,[\"category\", \"feature\"])\n",
    "        \n",
    "        # compute mean ocp and mse values.\n",
    "        # ocp: we use mean for each condition.\n",
    "        # mse: we use median for each condition\n",
    "        ocps = [eval_summary_by(cond_results, \"ocp\")[0] for cond_results in results_bycond.values()]\n",
    "        mses = [eval_summary_by(cond_results, \"mse\")[1] for cond_results in results_bycond.values()]\n",
    "\n",
    "        # compute mean and standard deviation over ocps and mses\n",
    "        this_dict[\"ocp_mean\"] = statistics.mean(ocps)\n",
    "        this_dict[\"ocp_sd\"] = statistics.stdev(ocps)\n",
    "        this_dict[\"msemed_mean\"] = statistics.mean(mses)\n",
    "        this_dict[\"msemed_sd\"] = statistics.stdev(mses)\n",
    "        \n",
    "        results_byparam.append(this_dict)\n",
    "        \n",
    "    return results_byparam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f83f4-de33-4e1b-a0e4-e04650488db0",
   "metadata": {},
   "source": [
    "# Making a development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6fe6d-20d2-43e2-8428-c46b1f6944be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = [f for f in os.listdir(grandratings_dir) if f.endswith(\"csv\")]\n",
    "\n",
    "import random\n",
    "random.seed(789)\n",
    "devset = random.sample(filenames, 6)\n",
    "[ filename[:-4].split(\"_\") for filename in devset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139eaec-fdec-4520-b210-04c91c32632a",
   "metadata": {},
   "source": [
    "# Determining hyperparameters\n",
    "\n",
    "## Fitted dimensions with seeds as words: offset, jitter\n",
    "\n",
    "First experiments looked like there was a lot of variance in result with different random seeds. To check into this, we run the hyperparameter tests n times with different random seeds. We then look at mean and standard deviation of the two measures we focus on: mean OC_p and median MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccab8bb-b9a0-460c-835f-ff115524d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_vals = [True, False]\n",
    "offset_vals= np.linspace(0.1, 2, num=20)\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "random.seed(5)\n",
    "randoms = [random.randrange(0,100) for _ in range(num_randseeds)]\n",
    "\n",
    "for randval in randoms:\n",
    "    for jval in jitter_vals:\n",
    "        for oval in offset_vals:\n",
    "\n",
    "            method = { \"method\": \"fitted_seedwords\",\n",
    "                      \"feature_dim\" : feature_dim,\n",
    "                      \"numfolds\" : 5,\n",
    "                      \"offset\" : oval,\n",
    "                      \"jitter\" : jval}\n",
    "\n",
    "\n",
    "            theseresults = crossvalidation(devset, method, word_vectors, grandratings_dir, grandfeatures_df, random_seed = randval)\n",
    "            \n",
    "            ocp_mean, _, _ = eval_summary_by(theseresults, \"ocp\")\n",
    "            _, mse_med, _ = eval_summary_by(theseresults, \"mse\")\n",
    "            print(oval, jval, ocp_mean, mse_med)\n",
    "            \n",
    "\n",
    "            for r in theseresults:\n",
    "                r.update({\"offset\":oval, \"j\": jval})\n",
    "                results[ (str(oval), str(jval))].append(r)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cd5a3-9cb4-4bf8-9a03-77b2e47d0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(eval_hyper(results, [\"offset\", \"j\"]))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edb4f3-99f9-478c-b99e-96e119ba53eb",
   "metadata": {},
   "source": [
    "We plot mean OC_P values, with standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee70047-8dc2-4e25-87f0-f510910443ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df[df.j == False].plot(y = \"ocp_mean\",x = \"offset\", yerr = \"ocp_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc3bfa-9a6d-4ee2-a2be-b7915fd5144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.j == True].plot(y = \"ocp_mean\", x = \"offset\", yerr = \"ocp_sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a80f68-6ce7-46af-990f-fbfe54a465e6",
   "metadata": {},
   "source": [
    "We plot mean values of median MSE, with standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4db552-541f-478b-a4d1-d5dd71d55739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.j == False].plot(y = \"msemed_mean\", x = \"offset\", yerr = \"msemed_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe414f6-d6e1-496b-b3f7-f2c77d19a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.j == True].plot(y = \"msemed_mean\", x = \"offset\", yerr = \"msemed_sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cbb0c-3fe5-4982-860c-b4fbac0f9e4e",
   "metadata": {},
   "source": [
    "### Parameters for seeds as words\n",
    "\n",
    "\n",
    "**BERT, last 4 layers**:\n",
    "\n",
    "The differences in performance are small compared to the error bars. So we go with an **offset of 1.0 with no jitter**, same as for GLoVE.  \n",
    "**RoBERTA, last 4 layers**:\n",
    "\n",
    "We again go with an **offset of 1.0 with no jitter**, for the same reason.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9e9e2-b17f-45a0-b618-06c01b010a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = \"ocp_mean\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0541c-7bcb-46fe-83b5-106b8da160b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = \"msemed_mean\", ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6a9b6-e6b3-4c67-a0b4-a9c02f226a71",
   "metadata": {},
   "source": [
    "## Fitted dimensions with seeds as dimensions: alpha, averaging\n",
    "\n",
    "We first run an exploration, with only one random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6afcc-ebc2-474c-8ac4-3beed7921995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_vals = [True, False]\n",
    "alpha_vals= np.linspace(0.001, 0.25, num=10)\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "randoms = [123]\n",
    "\n",
    "for randval in randoms:\n",
    "    for avgval in average_vals:\n",
    "        for alphaval in alpha_vals:\n",
    "\n",
    "            method = { \"method\": \"fitted_seeddims\",\n",
    "                      \"feature_dim\" : feature_dim,\n",
    "                      \"numfolds\" : 5,\n",
    "                      \"do_average\" : avgval,\n",
    "                      \"alpha\" : alphaval}\n",
    "\n",
    "\n",
    "            theseresults = crossvalidation(devset, method, word_vectors, grandratings_dir, grandfeatures_df, random_seed = randval)\n",
    "            \n",
    "            ocp_mean, _, _ = eval_summary_by(theseresults, \"ocp\")\n",
    "            _, mse_med, _ = eval_summary_by(theseresults, \"mse\")\n",
    "            print(alphaval, avgval, ocp_mean, mse_med)\n",
    "            \n",
    "            for r in theseresults:\n",
    "                r.update({\"alpha\":alphaval, \"avg\": avgval})\n",
    "                results[ (str(alphaval), str(avgval))].append(r)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ff7aa-b0e3-436c-92fd-51b8f30d907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(eval_hyper(results, [\"alpha\", \"avg\"]))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac3da0-eeb1-447a-8a01-ccdd4e24d465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == True].sort_values(by = \"alpha\").plot(x = \"alpha\", y = \"ocp_mean\", yerr = \"ocp_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492a999-08b0-4084-b397-6d74c4579cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == False].sort_values(by = \"alpha\").plot(x = \"alpha\", y = \"ocp_mean\", yerr = \"ocp_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051283d-d465-47bc-a3d3-347b656e2828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == True].sort_values(by = \"alpha\").plot(x = \"alpha\", y = \"msemed_mean\", yerr = \"msemed_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ced53-da85-4510-b083-cac6724c3755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == False].sort_values(by = \"alpha\").plot(x = \"alpha\", y = \"msemed_mean\", yerr = \"msemed_sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bf724-b81a-4ff7-a1b0-789e3efca8b7",
   "metadata": {},
   "source": [
    "We see that overall the performance is best, in terms of both OC_P and MSE, for small values of alpha, though again there is a large error bar. We explore the low range of alpha in more detail, both with and without averaging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b4878-1f15-4afd-a555-c85482a9e4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_vals = [True, False]\n",
    "alpha_vals= np.linspace(0.001, 0.15, num=10)\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "random.seed(5)\n",
    "randoms = [random.randrange(0,100) for _ in range(num_randseeds)]\n",
    "\n",
    "for randval in randoms:\n",
    "    for avgval in average_vals:\n",
    "        for alphaval in alpha_vals:\n",
    "\n",
    "            method = { \"method\": \"fitted_seeddims\",\n",
    "                      \"feature_dim\" : feature_dim,\n",
    "                      \"numfolds\" : 5,\n",
    "                      \"do_average\" : avgval,\n",
    "                      \"alpha\" : alphaval}\n",
    "\n",
    "\n",
    "            theseresults = crossvalidation(devset, method, word_vectors, grandratings_dir, grandfeatures_df, random_seed = randval)\n",
    "            \n",
    "            ocp_mean, _, _ = eval_summary_by(theseresults, \"ocp\")\n",
    "            _, mse_med, _ = eval_summary_by(theseresults, \"mse\")\n",
    "            print(alphaval, avgval, ocp_mean, mse_med)\n",
    "            \n",
    "            \n",
    "            for r in theseresults:\n",
    "                r.update({\"alpha\":alphaval, \"avg\": avgval})\n",
    "                results[ (str(alphaval), str(avgval))].append(r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c910fd-f481-4cf2-ac32-9522ecc71a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(eval_hyper(results, [\"alpha\", \"avg\"]))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb162516-c659-4e4c-a7df-515ce91b0141",
   "metadata": {},
   "source": [
    "We again plot average OC_P mean and MSE median values, with error bars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d505b2-c77e-437d-83bf-d37433647f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "df[df.avg == True].plot(y = \"ocp_mean\", x = \"alpha\", yerr = \"ocp_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ca7ea-87b0-4b0f-9ba4-f21c0ff5f592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == False].plot(y = \"ocp_mean\", x = \"alpha\", yerr = \"ocp_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54e721-8ad9-4b1c-a9c5-3c2da3f79032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == False].plot(y = \"msemed_mean\", x = \"alpha\", yerr = \"msemed_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13224ce9-da15-4be2-9ef1-29369a52bb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == True].plot(y = \"msemed_mean\", x = \"alpha\", yerr = \"msemed_sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c27ee-a06b-4ee0-a0d7-4103f08e0d3d",
   "metadata": {},
   "source": [
    "We again look at the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fc447-eb9a-4211-b6fd-a86e24b4f562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == True].sort_values(by = 'ocp_mean', ascending = False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf599502-3547-4bf6-9d35-e6a7f3ec443f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == False].sort_values(by = 'ocp_mean', ascending = False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb230197-1252-459c-94e9-0ce59eeace93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == False].sort_values(by = 'msemed_mean').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf928402-70cc-4d32-8f4d-37de594e7264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.avg == True].sort_values(by = 'msemed_mean').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0b537-c797-446b-bb02-8805eded681d",
   "metadata": {},
   "source": [
    "### Parameters for the fitted model with seed dimensions\n",
    "\n",
    "\n",
    "\n",
    "**BERT last 4 layers**\n",
    "\n",
    "Best performance is for lowest values of alpha. In terms of OC_P, it does not matter whether we average or not, but MSE values are lower with averaging, so we choose **alpha = 0.001, with averaging**\n",
    "\n",
    "**RoBERTA last 4 layers**\n",
    "\n",
    "Best performance is again for lowest values of alpha, and performance is slightly better with averaging so we again use **alpha = 0.001, with averaging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd82ca-b91b-4af4-af85-66c9361a455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f82c9-c795-4d87-9d03-d11f8c13cd75",
   "metadata": {},
   "source": [
    "# Combined model: seeds as words and dimensions\n",
    "\n",
    "We fix jitter, offset, and averaging to the best values determined above as they don't seem to have made much of a difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7c8a0-bde4-4698-b9c6-7f04494a8178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_vals = [True]\n",
    "alpha_vals= np.linspace(0.001, 0.15, num=10)\n",
    "jitter_vals = [False]\n",
    "offset_vals= [ 1.0]\n",
    "\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "random.seed(5)\n",
    "randoms = [random.randrange(0,100) for _ in range(num_randseeds)]\n",
    "\n",
    "for randval in randoms:\n",
    "    for avgval in average_vals:\n",
    "        for alphaval in alpha_vals:\n",
    "            for oval in offset_vals:\n",
    "                for jval in jitter_vals:\n",
    "\n",
    "                    method = { \"method\": \"combined\",\n",
    "                              \"feature_dim\" : feature_dim,\n",
    "                              \"numfolds\" : 5,\n",
    "                              \"do_average\" : avgval,\n",
    "                              \"alpha\" : alphaval,\n",
    "                              \"offset\" : oval,\n",
    "                              \"jitter\" : jval}\n",
    "\n",
    "\n",
    "                    theseresults = crossvalidation(devset, method, word_vectors, grandratings_dir, grandfeatures_df, random_seed = randval)\n",
    "                    \n",
    "                    ocp_mean, _, _ = eval_summary_by(theseresults, \"ocp\")\n",
    "                    _, mse_med, _ = eval_summary_by(theseresults, \"mse\")\n",
    "                    print(alphaval, avgval, oval, jval, ocp_mean, mse_med)\n",
    "            \n",
    "            \n",
    "                    for r in theseresults:\n",
    "                        r.update({\"alpha\":alphaval, \"avg\": avgval, \"offset\": oval, \"j\": jval})\n",
    "                        results[ (str(alphaval), str(avgval))].append(r) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe303de-8a2e-44ff-8c41-957e947e7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(eval_hyper(results, [\"alpha\", \"avg\", \"offset\", \"j\"]))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02484e3-9d14-4e93-9d48-c16b4ab8be45",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e91f6-655c-4d7e-a10c-3c087687798b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.plot(y = \"ocp_mean\", x = \"alpha\", yerr = \"ocp_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c3155-e9e8-4e48-8d4a-7d516b4667e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.plot(y = \"msemed_mean\", x = \"alpha\", yerr = \"msemed_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5909543-f49e-4a8c-a392-9a598b1b87ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = \"ocp_mean\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8d92c-7dae-4107-b242-4db38fca14ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = \"msemed_mean\", ascending = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ae24f-0c68-4d3f-a26c-27457f928761",
   "metadata": {},
   "source": [
    "### Parameters for the fitted model with seed words and seed dimensions\n",
    "\n",
    "\n",
    "**BERT last 4 layers**\n",
    "\n",
    "Both OC_P and MSE have a clear best value at **alpha = 0.02**. We use **averaging, no jitter, and an offset of 1.0**,\n",
    "\n",
    "**RoBERTA last 4 layers**\n",
    "\n",
    "Again, clear best value is **alpha = 0.02**. We use **averaging, no jitter, and an offset of 1.0**,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169a309-8abf-47b4-beb6-f5a63c90bc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
